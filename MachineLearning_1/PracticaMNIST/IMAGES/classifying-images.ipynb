{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using convnets with small datasets\n",
    "\n",
    "The first thing to do is download the following file https://lara.web.cern.ch/lara/train.zip in the jupyter terminal and uncompress it in the same folder as this notebook. \n",
    "\n",
    "To download another dataset from imagenet you can do it with the URL list of the images and using `wget -i`\n",
    "\n",
    "\n",
    "\n",
    "## Training a convnet from scratch\n",
    "\n",
    "Training an image classification model with very little data is a common situation you will find yourself in if you end up doing Computer Vision in a professional context.  \n",
    "\n",
    "Having \"few\" samples can mean anything from a few hundred to a few tens of thousands of images.  Let's illustrate a practical example here: let's focus on classifying images as \"dogs\" and \"cats\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of DEEP Learning in problems with few data\n",
    "\n",
    "You may have heard many times that Deep Learning only works when you have large amounts of data. This is partly true: one of the characteristics of Deep learning is that you can find interesting features from the training dataset itself, and this a priori is easier when you have many examples available, especially in the case of having input datasets with a high dimensionality, such as images.\n",
    "\n",
    "However, what constitutes a \"large\" dataset is relative. Specifically relative to the size and depth of the network we are trying to train. It is not possible to sand a convnet so that it becomes a complete problem with only a few dozen examples, but a few hundred can be enough if the model is well assembled (we will understand what \"well assembled\" means throughout the Deep Learning course).\n",
    "\n",
    "Since convnets learn local characteristics, invariant under translations, they are very efficient in terms of the number of images needed to carry out perceptual problems. So training a convnet from 0 with a not very large dataset can still lead to reasonable results as we will see here.\n",
    "\n",
    "But there is more: Deep Learning models are highly \"recyclable\". One can, for example, take an image classification problem and a trained speech-to-text converter on a very big dataset and then reuse it for solving a completely different problem only by adding some small modifications. More specifically, in the case of Computer Vision, many pre-trained models (usually trained on the ImageNet dataset) are made public so that one can download them and use them to create powerful Computer Vision models with very little data.\n",
    "\n",
    "But here we will just run a simple example.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos\n",
    "\n",
    "The cat vs dog dataset we use is not a Keras package. It was posted on Kaggle.com as part of a Computer Vision problem in late 2013, when ConvNets were not yet so popular. \n",
    "\n",
    "The images are medium resolution JPGEs. It looks like this:\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's no surprise that the 2013 Kaggle cat vs dog competition was won by ConvNets. The best were able to achieve up to 95% accuracy. In our example we are still far from this accuracy, but during the Deep Learning course we have learned how to approach this value using different methods to improve the performance of neural networks. It should be noted that in this example we are training on approximately only 10% of the data that was used for the contest. \n",
    "After downloading the dataset and decompressing it, we are going to create a new dataset containing three subsets: a training set containing 1000 images of each class, a validation set with 500 images of each class, and finally a test set with 500 images of each class.\n",
    "\n",
    "Here we have a few lines of code that make us this distribution automatically:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = '/home/nicolo/DataScienceMaster/MachineLearning_1/PracticaMNIST/train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = '/home/nicolo/DataScienceMaster/MachineLearning_1/PracticaMNIST/cats_and_dogs_small'\n",
    "#os.mkdir(base_dir, )\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "#os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "#os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "#os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "#os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "#os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "#os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "#os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "#os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "#os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training dog images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So effectively we have 2000 training images, 1000 validation images and 1000 test images. In each of these subsets there are the same number of examples from each class: this is what is called a balanced binary classification system, which means that our classification accuracy will be an adequate metric of the success of our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "In the above example we have built a small convnet to solve the problem of classifying handwritten numbers using the MNIST dataset, so we are already familiar with the terminology that keras uses. We are going to reuse the general structure we had in the previous example: our convnet will have a stack of alternate layers of `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "However, since we are dealing with larger images and a more complex problem, we will create our network accordingly: it will have one more layer of `Conv2D` + `MaxPooling2D`. This serves to increase the capacity of the network and to further reduce the size of the feature maps, so that they are not so huge when they reach the flattening step. We start using 150x150 input images (an arbitrary choice), and end up with feature maps that are 7x7 in size before the flattening layer.\n",
    "\n",
    "It is important to note that the depth of feature maps progressively increases as we move through the neural network (from 32 to 128) while the size of feature maps decreases (from 148x148 to 7x7). You will see this pattern in almost all convnets.\n",
    "\n",
    "As we are attacking a binary classification problem (dog or cat), we are going to finish the network with a single unit (a dense layer of size 1) and with a sigmoid activation. This unit will encode the probability that our network is looking at one class or another.\n",
    "\n",
    "The final look of the model should be as follows:\n",
    "\n",
    "\n",
    "![modelo_red_animales.png](https://github.com/laramaktub/MachineLearningI/blob/master/modelo_red_animales.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el paso de compilación utilizaremos el optimizador `RMSprop`(lr=1e-4). Como nuestra red termina con una única unidad sigmoide, vamos a utilizar binary crossentropy como nuestra función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "rms = optimizers.RMSprop(lr=1e-4)\n",
    "\n",
    "model.compile(optimizer = rms, #'rmsprop', \n",
    "              loss = \"binary_crossentropy\", \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data preprocessing\n",
    "\n",
    "The images must be properly formatted as float tensors before they are given to the net. That's just what we're going to do here. Before we pre-process them, the images are JPEG files. The steps to be able to give them to our network are roughly as follows:\n",
    "\n",
    "* Read the files with the images.\n",
    "* Decode the content of the JPEG in a \"grid\" with the RGB of the pixels \n",
    "* Turn that \"grill\" into floatation devices\n",
    "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval as neural networks prefer to work with small values. \n",
    "\n",
    "All this may seem very complicated but thanks to Keras our life is much easier and we can count on your tools to take care of these steps automatically. Keras has a module with tools for image processing, which can be found in `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` that allows us to automatically convert images we have on the hard disk into pre-processed tensors. This is exactly what we'll be using next. To do this we can use the flow_from_directory to take the images directly from the folders that we previously generated. We give it as input the folders where the training (or validation) images are, the size of the images (target_size), the size of the batch we're going to use (we're going to start with 20) and as there are only two categories, we tell it that we're going to use binary_crossentropy (class_mode). When we run these commands we'll get the following, the total number of images and how many classes there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(150, 150),\n",
    "                                                  batch_size=20,\n",
    "                                                  class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                              target_size=(150, 150),\n",
    "                                                              batch_size=20,\n",
    "                                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these generators: it takes us to a batch of 150x150 RGB images (dimensions `(20, 150, 150, 3)`) and binary tags (dimension `(20,)`). 20 is the number of examples in each batch (what we call the batch size). The generator generates these batches indefinitely: it runs a loop endlessly through all the images we have in the folder. That's why we have to type `break` to break the loop at some point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the fit. In this case, as what we have is a generator, we use fit_generator. We are going to run 30 epochs and use the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs = 30,\n",
    "                              validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's a nice idea to save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-754ac3b42547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net_images.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('net_images.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate our model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 0.763649582862854\n",
      "Test accuracy: 0.734000027179718\n"
     ]
    }
   ],
   "source": [
    "# Cargo el modelo, ya que lo he guardado\n",
    "\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model('net_images.h5')\n",
    "\n",
    "# miro si de verdad estoy cargando el modelo que he guardado antes \n",
    "loaded_model.summary()\n",
    "\n",
    "test_loss, test_acc = loaded_model.evaluate(test_generator, verbose = 0)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to optimize the network with the tools you have learnt during the lesson. Try to make improvements both in terms of speed and accuracy. Comment the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento mejorar el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratandose de un dataset con pocas imagenes, voy a intentar utilizar técnicas de data augmentation para que la red tenga más eventos para entrenar:\n",
    "- horizontal flip;\n",
    "- vertical flip;\n",
    "- rotation;\n",
    "- zoom\n",
    "\n",
    "Intento usar un learning rate más alto, para acercarme al mejor valor de la accuracy más rápidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen2 = ImageDataGenerator(rescale=1./255,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=90,\n",
    "                                  zoom_range=[0.5,1.0])\n",
    "\n",
    "test_datagen2 = ImageDataGenerator(rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True,\n",
    "                                 rotation_range=90,\n",
    "                                 zoom_range=[0.5,1.0])\n",
    "\n",
    "validation_datagen2 = ImageDataGenerator(rescale=1./255,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True,\n",
    "                                       rotation_range=90,\n",
    "                                       zoom_range=[0.5,1.0])\n",
    "\n",
    "train_generator2 = train_datagen2.flow_from_directory(train_dir,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "test_generator2 = test_datagen2.flow_from_directory(test_dir,\n",
    "                                                  target_size=(150, 150),\n",
    "                                                  batch_size=20,\n",
    "                                                  class_mode='binary')\n",
    "\n",
    "validation_generator2 = validation_datagen2.flow_from_directory(validation_dir,\n",
    "                                                              target_size=(150, 150),\n",
    "                                                              batch_size=20,\n",
    "                                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "\n",
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model2.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model2.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model2.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "\n",
    "model2.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "              \n",
    "rms = optimizers.RMSprop(lr=5e-4)\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', patience = 5,\n",
    "                  restore_best_weights = True)\n",
    "\n",
    "model2.compile(optimizer = rms,             \n",
    "               loss = \"binary_crossentropy\", \n",
    "               metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 0.7138 - acc: 0.5105 - val_loss: 0.6866 - val_acc: 0.5040\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.6867 - acc: 0.5710 - val_loss: 0.7297 - val_acc: 0.5820\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.6680 - acc: 0.5985 - val_loss: 0.5971 - val_acc: 0.6370\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.6379 - acc: 0.6405 - val_loss: 0.6094 - val_acc: 0.6450\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.6446 - acc: 0.6575 - val_loss: 0.6083 - val_acc: 0.6410\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.6155 - acc: 0.6625 - val_loss: 0.5694 - val_acc: 0.6830\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 42s 422ms/step - loss: 0.6134 - acc: 0.6665 - val_loss: 0.6371 - val_acc: 0.6760\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 0.5932 - acc: 0.6935 - val_loss: 0.6002 - val_acc: 0.6670\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 42s 422ms/step - loss: 0.5950 - acc: 0.6885 - val_loss: 0.4880 - val_acc: 0.6880\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.5774 - acc: 0.7010 - val_loss: 0.6026 - val_acc: 0.6880\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.5768 - acc: 0.7155 - val_loss: 0.6382 - val_acc: 0.6660\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.5740 - acc: 0.7025 - val_loss: 0.7294 - val_acc: 0.6890\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.5650 - acc: 0.7115 - val_loss: 0.5764 - val_acc: 0.6770\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 42s 422ms/step - loss: 0.5619 - acc: 0.7140 - val_loss: 0.6350 - val_acc: 0.6940\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.5535 - acc: 0.7150 - val_loss: 0.6117 - val_acc: 0.7200\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.5615 - acc: 0.7305 - val_loss: 0.3917 - val_acc: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 42s 422ms/step - loss: 0.5572 - acc: 0.7340 - val_loss: 0.4584 - val_acc: 0.6990\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.5460 - acc: 0.7380 - val_loss: 0.4546 - val_acc: 0.7240\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.5365 - acc: 0.7315 - val_loss: 0.5701 - val_acc: 0.7210\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 42s 421ms/step - loss: 0.5416 - acc: 0.7170 - val_loss: 0.6227 - val_acc: 0.7000\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.5371 - acc: 0.7335 - val_loss: 0.4719 - val_acc: 0.7130\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 43s 425ms/step - loss: 0.5391 - acc: 0.7385 - val_loss: 0.3868 - val_acc: 0.7150\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.5231 - acc: 0.7455 - val_loss: 0.4203 - val_acc: 0.7050\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit_generator(train_generator2,\n",
    "                                epochs = 30,\n",
    "                                validation_data = validation_generator2,\n",
    "                                callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.9180858731269836\n",
      "Test accuracy: 0.7099999785423279\n"
     ]
    }
   ],
   "source": [
    "# Miro la accuracy en el test\n",
    "test_loss2, test_acc2 = model2.evaluate(test_generator2, verbose = 0)\n",
    "print('Test loss:', test_loss2)\n",
    "print('Test accuracy:', test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('my_net_images.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
