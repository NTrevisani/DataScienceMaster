{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to ConvNets: Classifying handwritten numbers\n",
    "\n",
    "\n",
    "Let's take a look at a simple example of a convnet. We will use it to classify the MNIST dataset, which is an open dataset containing handwritten numbers. \n",
    "\n",
    "![Handwritten numbers from the MNIST dataset](http://corochann.com/wp-content/uploads/2017/02/mnist_plot.png)\n",
    "\n",
    "Let's create a first basic convnet. It's a stack of 'Conv2D' and 'MaxPooling2D' layers. \n",
    "The important thing to note is that a convnet takes as input tensors of size `(image_height, image_width, image_channels)`. \n",
    "To do this we must first find out the size of the images in our dataset. \n",
    "\n",
    "The network must have the following layers:\n",
    "\n",
    "- A convolutional layer (Conv2D) with 32 3x3 filters and relu activation. In this first layer you must indicate the size of the input (input_shape).\n",
    "- A second layer of Max Pooling (MaxPooling2D) of 2x2\n",
    "- A third convolutional layer with 64 3x3 filters and relu activation\n",
    "- A fourth layer of 2x2 Max Pooling (MaxPooling2D)\n",
    "- A fifth convolutional layer of 64 3x3 filters and relu activation\n",
    "\n",
    "You'll know you've done it right when the model.summary() output is:\n",
    "\n",
    "![imagen_output.png](https://github.com/laramaktub/MachineLearningI/blob/master/imagen_output.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see above that the output of each Conv2D and MaxPooling2D layer is a 3D tensor of dimensions (height, width, channels). The width and height tend to decrease as we go deeper into the network. The number of channels is controlled by the first argument passed to the Conv2D layers (e.g. 32 or 64).\n",
    "\n",
    "The next step would be to give our last tensor (of dimensions (3, 3, 64)) as input to a densely connected network. These classifiers process vectors, which are 1D, while our output is a 3D tensor. So first we will have to flatten our 3D output and convert it to 1D and then add a few dense layers:\n",
    "\n",
    "- First flatten the output.\n",
    "- Add a first layer of 64 neurons and relu activation\n",
    "- Add a last layer of 10 neurons (as many as you can sort) and softmax activation\n",
    "- You'll know you've done well when the summary looks like this:\n",
    "\n",
    "![imagen_output_flat.png](https://github.com/laramaktub/MachineLearningI/blob/master/imagen_output_flat.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our dimensional output `(3, 3, 64)` has been flattened into a vector of dimension `(576,)`, before entering the two dense layers.\n",
    "\n",
    "We are now going to train our network with the images from the MNIST dataset.\n",
    "\n",
    "We then load the dataset and put it into vectors: train_images, train_labels, test_images, test_labels\n",
    "\n",
    "Before you continue, print:\n",
    "\n",
    "- What is the size of the training dataset?\n",
    "- What does the training dataset look like?\n",
    "- What do the training labels look like?\n",
    "- Print the fourth image of the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training dataset is: 60000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADuCAYAAAAgAly4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2debhVVf3/Xx/BGUEQQXIATUixnOevP6TEIbQQTdPMME18Mkt9tCQ106JwKL8PaQ44ovJIPomKlpmhOOsXLc2BUUMlUQIHEAcC1++Pc9a663DP5Zx7ztn77H3v+/U857mf8zl7+Oz32XedtdfwWeacQwghRP5Yq9kBCCGEqA0V4EIIkVNUgAshRE5RAS6EEDlFBbgQQuQUFeBCCJFTMlmAm9l8MxtW5bbOzLat8Tw175tXpG2ySN/kkLatyWQBnmfMbLqZfWJmHxZfs5sdU0fBzHqZ2V1mttzMXjezbzU7po6ImQ0s3sO3NTuWjoKZnWZmz5rZp2Z2c6OO27VRBxIlnOacu77ZQXRAfg+sAPoCOwN/MrMXnHMvNzesDsfvgRnNDqKD8RYwFjgYWL9RB818DdzM9jSzp8zsfTNbaGZXmtk6q2023MxeM7PFZnaZma0V7X+imc00s/fM7AEz65/yJWSWPGlrZhsCRwI/c8596Jx7HJgKHJ/UOeslT/pG5zwGeB+YlvS56iFv2jrnpjjn7gaWNPK4mS/AgVXAmUBvYB/gAODU1bYZCewO7AqMAE4EMLPDgXOBI4BNgceA26s5qZldVbw5yr3+WWH3ccWb5gkzG1rVVTaHPGk7CFjlnJsT+V4AdqjmnE0iT/piZt2BXwBnteMam0WutE0M51zmXsB8YFgbn50B3BW9d8Ah0ftTgWlF+37gpOiztYCPgP7Rvts2OPa9gI2AdYFRwDLg883WNO/aAv8PeHs138nA9GZr2hH0LR5zPHBO0b4QuK3ZenYUbaNzjQVubtTxMl8DN7NBZnafmb1tZkuBX1P41Y15M7JfBz5XtPsD4/0vJPAuYMDmScXrnHvGObfMOfepc24i8AQwPKnz1UPOtP0Q6L6arzuFH8hMkid9zWxnYBjwv0kcv9HkSdskyXwBDlwNzAIGOue6U3j0sdW22TKyt6LQYQCFL/AU59zG0Wt959yTlU5qZtdYy0iS1V/t6TRzZeLNCnnSdg7Q1cwGRr6dgCx3YOZJ36HAAOANM3sbOBs40sz+Xu3FpkyetE2OZj8WVXpUAv4PuIDCl7MdMBt4fLVHpWlATwpf2CxgdPGzkcBLwA7F9z2Ao1bbt5GP+RtT6GVej8IIn+OA5cAXmq1p3rUtHnMyhbbKDYH/AT7w58/KK6/6AhsAm0Wv3wB/BDZttqZ517Z4zK7FcmEccKsvI+o+brO/lCq+qCFF8T+k0NnwizJf1I+A1yj08P4W6BJ9fjzwIrCUwi/vjUl9URQ6RGZQeKx/H3gaOLDZenYEbYvH7AXcTeFH8Q3gW83WsyPpu9p1XEiG28Dzpm1RT7fa68J6j2vFgwshhMgZeWgDF0IIUQYV4EIIkVPqKsDN7BAzm21m88xsTKOCEgWkb3JI2+SQtulRcxu4mXWhMLTrQGABhc67Y51zrzQuvM6L9E0OaZsc0jZd6klmtScwzzn3GoCZTaYwXbXNL8rM1GNamcXOuU1pp77Stipq0ra4jfStgHPOkLZJ4e/dEuppQtmc0plOC8jhTKYM8nrxr/RtPNI2eaRtMrxezllPDbzc7MJWv6RmNhoYXcd5OisV9ZW2NaN7NzmkbYrUU4AvoHSq6ha0TFUNOOcmABNAj0rtpKK+0rZmdO8mh7RNkXqaUGYAA81sayvk4T2GQn5m0Rikb3JI2+SQtilScw3cObfSzE4DHgC6UJiKmuXEQrlC+iaHtE0OaZsuqU6l16NSVTznnNu9vTtJ26qoSVuQvtVQHIXSbqRtVZS9dzUTUwghcooKcCGEyCkqwIUQIqfUM4xQiKrYbbfdgn3aaacB8J3vfCf4brnllmBfccUVAPz971ldCEaI7KAauBBC5BQV4EIIkVM6zTDCLl26BLtHjx5r3NY/5m+wwQbB94UvfCHYP/jBDwD4zW9+E3zHHnssAJ988knwXXzxxcG+6KKLqg21Qwwj3HnnnYP90EMPBbt799UXli/lgw8+AGCTTTZJIiwNIyxywAEHBHvSpEkA7L///sE3e/bsdh+zMw4jPP/884Pt/8fXWqulXjx06FAAHnnkkXpPpWGEQgjRkVABLoQQOSX3o1C22morANZZZ53g23fffYO93377AbDxxhsH35FHHtnu8yxYsCDYv/vd7wAYOXJk8C1btgyAF154Ifga8NiUO/bcc08A7rzzzuCLm6x8k53XC2DFihXB9k0ne++9d/D5ESnxdlllyJAhwY6bge66665mhNMme+yxR7BnzJjRxEjyxwknnBDsc845J9ifffZZq22TbqJWDVwIIXJKLmvg5TrIKnVM1kL8ixp3Vnz44YdAS+cPwMKFCwF47733gq+WjqA84Tt5d9111+C77bbbAOjXr98a9507d26wL7300mBPnjwZgCeeeCL4vPbjxo2rM+Lk8Z1WAAMHDgx2VmrgvoNt6623Dr7+/fsDYFZTH2Snw+sFsN566zUxEtXAhRAit6gAF0KInJLLJpQ33ngj2EuWLAFqa0J55plngv3+++8H+8tf/jJQ2ml26623tvv4HZ1rr70WaBkD3x7iZpdu3boF23f8xk0RO+64Y40Rpk+cIuCpp55qYiTl8U1bJ598cvD5Zq9Zs2Y1Jaa8MGzYMAB++MMflv3c63fYYYcF3zvvvJNoTKqBCyFETsllDfzdd98N9o9//GOg9FfvH//4R7D9kL+Y559/HoADDzww+JYvXx7sHXbYAYDTTz+9QRF3HOLEVIceeihQvvMrHkJ57733BtvPXn3rrZZlEuPvy3cCf+UrXwm+PHWuxbPwssj111/fyhd3KItS/DBkgJtuuglo+2n/sssuA+D118suIJ8IFe82M7vRzBaZ2UuRr5eZPWhmc4t/eyYbZudC+iaHtE0OaZs+1VQXbgYOWc03BpjmnBsITCu+F41D+iaHtE0OaZsyVSWzMrMBwH3OuS8W388GhjrnFppZP2C6c+4LaziEP05i05LiJEnxLD/f0XbSSScF37e//W0Abr/99qTCqYfngI1op75JatuexFT3338/UNqxGSdJ8h2S8aP8f/7zn1bHWbVqVbA/+uijVsepMV94TdpCZX39dcUdl1OmTAn28ccfX0u8DefJJ58ESme6+pnLTz/9dL2Hn0PGyoV6ue6664J94okntvp8+vTpwY4ThCVAQ5NZ9XXOLQQo/u1TT2SiFdI3OaRtckjblEm8E9PMRgOjkz5PZ0TaJov0TQ5p2xhqLcDfMbN+0aPSorY2dM5NACZAso9KS5cuLev3+aVj/BjYP/zhD8FXLhFNE6lK36S1HTRoENAy0gdKe+AXL14MtKQRAJg4cSLQkm4A4E9/+lNZu1rWX399AM4666zgO+6449p9nCKJ3LvDhw8viTVL9O3bN9jxFHrPv//970adKnPlQi307t072HGziS8j4jkjY8eOTS+wMtTahDIVGFW0RwH3NCYcUUT6Joe0TQ5pmzIVa+BmdjswFOhtZguAnwMXA3eY2UnAG8BRSQZZDxdeeCFQOn7Zd4b5mVUAf/3rX1ONaw30pon6rrvuusH2Y7Z97RJKO4j9rMNnn302+JKsgfrUwXWQmLbxik2el19+uRGHrpt45ShfG58zZ07wxd9pneSmXCjHgAEDgNJUyOXwC28DPPzww0mGVJGKBbhzrq150ol2uXZiFjvnliB9k0DaJoi0TZ9sTxsTQgjRJrmcSt8e/BT5OHmPH0Mcj/GMH4V8k8Dvf//74Etz8edmsssuuwQ7bjrxjBgxItidccWh9pDWSjfxePxDDinMufNzHQAOOuigVvv88pe/DHbcKdeZ8dq1lTxt2rRpAIwfPz61mCqhGrgQQuSUDl8D97z66qvB9mva+eQ0UDpTztsbbrhh8N1yyy3BjofNdTQuv/zyYPskUnFNO61ad5wUKmNDPKumV69eVW230047BTtO3OU72bfYYovg82u/xsMoY60+/vhjoDRV8qeffhrsrl0L//LPPfdcVbF1dA4//PBgX3zxxa0+f/zxx4M9alRhgE25ocnNQjVwIYTIKSrAhRAip3SaJpQYv8BsnAc5bjrwSWl+/etfB1+8kOmvfvUroKEz2JqOz6ceJ67yHbdTp05NPZ642cTH4fO4ZxHfdBF3dl9zzTXBPvfcc9vcN+40i5tQVq5cCbQk8wJ45ZVXALjxxhuDLx6H75u44pVgFixYEGw/Tr+zr75T7Zjv1157LdhJr65TC6qBCyFETlEBLoQQOaVTNqF4XnopLDLE0UcfHeyvfe1rQOkolVNOOSXYAwcOBEqXZMs7/tHaj3IAWLSokIsoTvqVBPH0fZ/6IMbnIP/pT3+aaBz1cOqppwKly2n5PNuViBfpvvvuu4M9c+ZMoLY83aNHtyT623TTTYMdNwl0Zs455xyg8ginciNTsoRq4EIIkVM6dQ08Jp6NduuttwKlq8b48bMAQ4YMAWDo0KHBF6/M0VHw44eTGPce17rPP//8YPvUtXHH229/+1ugNEVtVrnkkkuaHQLQ9uowlTrtOjJxB3252amee+5pSaI4e/bsRGOqF9XAhRAip6gAF0KInNKpm1Di8bff+MY3gr3HHnsApc0mMX4s7qOPPppgdM0nifHf/jE2XuXnm9/8ZrD94+uRRx7Z8HOLljkQnZE453/Pnj1bfe47i32qjTygGrgQQuQUFeBCCJFTqllSbUvgFmAz4DNggnNuvJn1Av4ADADmA0c7595LLtT6iJe8Ou200wA44ogjgm+zzTZb4/6rVq0Kth+VkVSWvGZo66dwx1O5faa2008/va5jn3nmmcH+2c9+BpQujjxp0qRg+2XaEqIPNEffzkKWtd1kk02CXe5/96qrrgLyMdrJU00NfCVwlnNue2Bv4AdmNhgYA0xzzg0EphXfi/pZD2mbFH107yaHtE2fatbEXAgsLNrLzGwmsDkwgsJixwATgenAOYlE2U7i2vSxxxaW9PS1bmhJZFOJOEmQT2AFiSd3WocmaOuTMMXJmLyOv/vd74IvTqK0ZMkSAPbee+/g87nU4xzXcT5rP+vwgQceCD5f80mBj8n4vZsE8VPVoEGDgNpmd1ZB5rSNZ1PHedPL8eSTTyYdTsNpVxu4mQ0AdgGeAfoWC3dfyPdpdHCdlA+RtkmxAbp3k0TapkzVwwjNrBtwJ3CGc25p/KteYb/RwOiKGwpP1Q3r0rbdvKl7NzmkbfpUVYCb2doUCu9JzrkpRfc7ZtbPObfQzPoBi8rt65ybAEwoHqfhKwP37dsXgMGDBwfflVdeGeztttuuquPES1BddtllQOmU2pSX9cqEtl26dAFaEjVB6fjspUuXAi3JvdoifjT1i0dfcMEFDYuzHfh8CZnQNy3iZrFKzQgNIBPa+vkGflk6KP0fXrFiBVC6cHkW831XouK3aYWf1BuAmc65y6OPpgKjivYo4J7V9xU1I22TRfomh7RNkWpq4P8DHA+8aGZ+SZRzgYuBO8zsJOAN4KhkQmzBLxJ77bXXBp//pd1mm22qPo6vEfokSVDaqeZXV2kSPWiCtk899RQAM2bMCD4/IzUm7iD2Tz8xvmNz8uTJwVfvMMQGMtjMhtMEfbPCPvvsA8DNN9/c8GNnSduNN94YaHt4sF9N6+yzz04tpiSoZhTK40BbDVvlU56JevjAObcEaZsErzjn/ly0pW+Dkbbpo5mYQgiRUzKZzGqvvfYKdpz0aM899wRg8803r/pYfkHYeCyzX6x4+fLldcXZkfD5t+PZqX4VojhfdznGjx8f7KuvvhqAefPmNTpEUSPVjgwR+UM1cCGEyCkqwIUQIqdksgll5MiRZe1y+Nzc9913X/CtXLky2H6kSbxkmmibePk0v8BwuYWGRba5//77g33UUZ1mkE1g1qxZQOkchP32269Z4SSGauBCCJFTLJ6llfjJcjybLUWec87t3t6dpG1V1KQtSN9qcM7V1Fsqbaui7L2rGrgQQuQUFeBCCJFTVIALIUROUQEuhBA5RQW4EELkFBXgQgiRU1SACyFETkl7JuZiYHnxb0ehN429nv417idtK1OrtiB9KyFtS0nl3k11Ig+AmT1b62SKLJKl68lSLI0ga9eTtXjqJUvXk6VYGkFa16MmFCGEyCkqwIUQIqc0owCf0IRzJkmWridLsTSCrF1P1uKplyxdT5ZiaQSpXE/qbeBCCCEag5pQhBAip6gAF0KInJJqAW5mh5jZbDObZ2Zj0jx3IzCzLc3sYTObaWYvm9npRX8vM3vQzOYW//ZsQmzSNrnYpG2y8UnfWnHOpfICugCvAtsA6wAvAIPTOn+DrqEfsGvR3giYAwwGLgXGFP1jgEtSjkvaStvcaSt963+lWQPfE5jnnHvNObcCmAyMSPH8deOcW+ic+3vRXgbMBDancB0Ti5tNBA5POTRpmxzSNlmkbx2kWYBvDrwZvV9Q9OUSMxsA7AI8A/R1zi2EwpcJ9Ek5HGmbHNI2WaRvHaRZgJdbLy+XYxjNrBtwJ3CGc25ps+NB2iaJtE0W6VsHaRbgC4Ato/dbAG+leP6GYGZrU/iSJjnnphTd75hZv+Ln/YBFKYclbZND2iaL9K2DNAvwGcBAM9vazNYBjgGmpnj+ujEzA24AZjrnLo8+mgqMKtqjgHtSDk3aJoe0TRbpWw8p99YOp9BD+ypwXrN7j2uIfz8Kj3f/BJ4vvoYDmwDTgLnFv72aEJu0lba501b61vfSVHohhMgpmokphBA5RQW4EELkFBXgQgiRU1SACyFETlEBLoQQOUUFuBBC5BQV4EIIkVNUgAshRE5RAS6EEDlFBbgQQuQUFeBCCJFTVIALIUROUQEuhBA5RQW4EELklEwW4GY238yGVbmtM7NtazxPzfvmFWmbLNI3OaRtazJZgOcZM9vezB4ysw/MbJ6ZjWx2TB0BM1vXzG4ws9fNbJmZ/cPMvtrsuDoSZnaamT1rZp+a2c3NjqcjYWa3mdlCM1tqZnPM7HuNOK4K8AZiZl0pLJt0H9ALGA3cZmaDmhpYx6ArhdXL9wd6AD8D7iiuAi4aw1vAWODGZgfSARkHDHDOdQe+Dow1s93qPWjmC3Az29PMnjKz94u/YFcW186LGW5mr5nZYjO7zMzWivY/0cxmmtl7ZvaAmfVPMNztgM8B/+ucW+Wcewh4Ajg+wXPWTJ60dc4td85d6Jyb75z7zDl3H/AvoO5/gqTIk74Azrkpzrm7gSVJnqcR5FDbl51zn/q3xdfn6z1u5gtwYBVwJtAb2Ac4ADh1tW1GArsDuwIjgBMBzOxw4FzgCGBT4DHg9mpOamZXFW+Ocq9/trVbG74vVnPOJpAnbVc/Rl9gEPByNds3idzqmwNyp21x34+AWcBC4M/VXeoaaPaCoG0sEjofGNbGZ2cAd0XvHXBI9P5UYFrRvh84KfpsLeAjoH+077YNjHtt4DXgJ0X7IGAF8ECzNc27tmV0/htwbbP17KD6jgVubraWHVTbLhQWQT4fWLve42W+Bm5mg8zsPjN728yWAr+m8Ksb82Zkv06hGQOgPzDe/0IC71KoEW+eRKzOuf8ChwOHAm8DZwF3AAuSOF+95EnbKOa1gFsp/DCeluS56iWP+uaFvGrrCk2rjwNbAN+v93iZL8CBqyk8cgx0hQ6Ac2ndVLFlZG9FoTMGCl/gKc65jaPX+s65Jyud1MyuMbMP23i1+djunPunc25/59wmzrmDgW2A/2vH9aZJrrQ1MwNuAPoCRxZ/MLNMrvTNGXnXtisNaANv+mNRpUclCoXfBRS+nO2A2cDjqz0qTQN6UvjCZgGji5+NBF4Cdii+7wEctdq+DX1UAnYE1gM2AM6m0NG2brM17SDaXgM8DXRrto4dVN+uxXt3HIWnnPWArs3WNO/aAn2AY4BuFJpQDgaWAyPqPnazv5QqvqghRfE/pNDZ8IsyX9SPKLQ9LwF+C3SJPj8eeBFYSuGX98akvqjiMS8D3ivGe3+jj99ZtaXw2OuAT4rx+tdxzda0I+hbPOaFtIyQ8K8Lm61p3rWl0FH6CPB+8XwvAic34thWPIEQQoickYc2cCGEEGVQAS6EEDmlrgLczA4xs9lWyPkxplFBiQLSNzmkbXJI2/SouQ3czLoAc4ADKYxzngEc65x7pXHhdV6kb3JI2+SQtunStY599wTmOedeAzCzyRSmq7b5RZmZekwrs9g5tynt1FfaVkVN2ha3kb4VcM4Z0jYp/L1bQj1NKJtTOtNpAWVmMpnZaCukqHy2jnN1Jl4v/q2or7RtN1VrC9K3RqRtMrxezllPDbxc4qZWv6TOuQnABNAvbTupqK+0rRndu8khbVOknhr4Akqnqm5By1RVUT/SNzmkbXJI2xSppwCfAQw0s62tkIf3GGBqY8ISSN8kkbbJIW1TpOYmFOfcSjM7DXiAwvz+G51zHSVRTtORvskhbZND2qZLqlPp1dZVFc8553Zv707Stipq0hakbzUUR6G0G2lbFWXvXc3EFEKInKICXAghcooKcCGEyCn1jAMXQqTA+PHjg/2jH/0IgJdeein4DjvssGC//nrZ+R6ig6IauBBC5BQV4EIIkVPUhCISZ6ONNgp2t27dADj00EODb9NNW3L0XH755QB8+umnKUWXXQYMGADAt7/97eD77LPPANh+++2Db7vttgu2mlCqY9CgQQCsvfbawTdkyBAArrrqquDzereHe+65J9jHHHMMACtWrKgpzkqoBi6EEDlFBbgQQuQUNaGIhuIf+88555zg22effYL9xS9+cY379+vXD2gZbdGZ+c9//gPAo48+Gnxf//rXmxVOLtlhhx2CfcIJJwT7qKOOAmCttVrqsJ/73OeA0maTWmaqx9/RNddcA8AZZ5wRfEuXLm33MdtCNXAhhMgpnaYGvtdeewXbdwrtv//+wRf/UnvOPvvsYL/1VktGzP322w+A2267LfieeeaZxgWbE3znWVy7OO644wBYf/31g8+sJUXGm28Wcv0vW7Ys+OIOuaOPPhoo7UiaNWtWI8PODcuXLwfUMVkP48aNC/bw4cNTP/93vvMdAG644Ybge+KJJxp2fNXAhRAip6gAF0KInNLhm1C++c1vAqXTkXv37g2UPtpPnz492H5c8mWXXVb2mH6/ePyyH+/ZEenRo0ewL7nkkmB7beNx3uWYO3dusA8++GCgdPxt3ETivxv/tzOz8cYbA7DTTjs1OZL88uCDDwa7XBPKokWLgu2bOeKOzXLjwPfdd99gx82wzUA1cCGEyCkdpgbetWvLpey+e0ve8+uuuw6ADTbYIPj8sKxf/vKXwff4448He9111wXgjjvuCL6DDjqo1TmffbZzLKg9cuTIYH/ve9+rap9XX3012AceeGCwfSfmtttu26DoOi7+nt1qq63WuN0ee+wRbP80o47PAldffXWw77777laf//e//w3222+/XdUxu3fvHuw4qZgfhhjjz5lUWVGxBm5mN5rZIjN7KfL1MrMHzWxu8W/PRKLrpEjf5JC2ySFt06eaJpSbgUNW840BpjnnBgLTiu9F45C+ySFtk0PapkzFJhTn3KNmNmA19whgaNGeCEwHzqGJxAl/rr/++lafx50ZvvOtrRlR/vNyzSYACxYsAGDixIm1BVuZTOnrZ621xfz584M9Y8YMoHQmpm82iYnHfqdMprRdE37uwc033xx8F154YavtYt/7778PwJVXXplkaG2ROW1XrlwZ7HL3YS34jniAnj3X/JDhy4qkkrPV2gbe1zm3EMA5t9DM+rS1oZmNBkbXeJ7OSlX6Stua0L2bHNI2ZRLvxHTOTQAmgFafbjTSNlmkb3JI28ZQawH+jpn1K/7K9gMWVdwjIfxIknPPPTf44gQ0fkr2+eefH3yVksmcd955a/zcJ1ryyYYSIDP6Apx88snBHj26pdL017/+FYB58+YFXzyudk307du3QdG1m0xpWw3xaKlyTSgZInfatgc/1yP+f4hTRpTjggsuSDSmWseBTwVGFe1RwD1r2Fa0H+mbHNI2OaRtylSsgZvZ7RQ6Jnqb2QLg58DFwB1mdhLwBrDmXq4GE/+q+Zp3vOLFAw88EGzfmfbxxx+3Os56660X7LjD0o+7jWdqjh07NtjxihsJ0Jsm67s6cSKvRtUA4xSzKZI5bduLnyVYy0oxKZBrbT0+IRvAmDEtA2n83IV4FnE5nn/++WDH48yToJpRKMe28dEBDY5FFFjsnFuC9E0CaZsg0jZ9NJVeCCFySq6m0vvkPqeeemrw+Q7LuNnk8MMPX+Nx/KPQpEmTgm+33XZrtd0f//jHYF966aU1RNz58B28G2644Rq3+9KXvlTW/+STTwLw1FNPNTawDoJvOqllpZjOiF8hCuD4448P9rBhw9rcx+f7h8o6xwMifHPLn//85+Ar13TbSFQDF0KInJKrGvg666wDlE81Gq+h2KdPy/yB7373u0DpOnV+XcZu3boFX/xL6+14xR2/OkpnJ04KNnjwYAB+/vOfB1+5lJ2V0nPGnaT++1q1alX9wYpOi/8fnzp1avBVSgpWC4899liwJ0yY0PDjV0I1cCGEyCkqwIUQIqfkqgnFj/WOZ0D6VXH+9a9/BV+ljgf/yB53QPTr1y/YixcvBuDee++tM+J848e77rLLLsF35513BttrFnfUeG3jTshDDmlJZhk3wXjiXO5HHHEEULqCUjzGX4j2EM/liO01UanJL+awww4L9le/+lUA7r///vaEWBeqgQshRE5RAS6EEDklV00oPtdxPM77vvvuA6BXr17BFy/n5ae9xzmV3333XQAmT54cfHETSuzvbPiRPtDS9DFlypSy21500UUAPPTQQ8H3xBNPAKXfR/y5Hx0QEy8OPW7cOADeeOON4PPLUiWVUzlPVJpKP2TIEKBp+cAzg1/qbOjQocEXrxng54188sknVR/zpJNOAuCHP/xhAyJsDKqBCyFETrE0Z3RlJe+vr6U88sgjwRfXaM4445IBtaUAAAejSURBVAwArrjiinQDK/Ccc273ypuVUq+2vsPyF7/4RfD9+Mc/brVd3EHjZ7b5JyNoqU3Hs9F23XXXYPsOyXhma1wrHzFiRKtz/u1vfwPgkksuCb733nuv1XZxEqE2qElbyM6968fHV/q/3XHHHYP9yiuvJBqTxzlXXS/hamRF20r06NEDgCVLlpT9/Gtf+xqQWCdm2XtXNXAhhMgpKsCFECKn5KoTs1H4VTTiZpP4kbSzdGJ26dIl2H7Vl7PPPjv4fPqAOCdyrI1vOtl995YnO995Fo8dnzt3brC///3vA/Dwww8HX/fu3YO97777AqU5mX0ahHhh6hi/WO3WW29d9vOOxDXXXAPAKaecssbt4pWTfJOgqI94MeOsoBq4EELkFBXgQgiRU6pZUm1L4BZgM+AzYIJzbryZ9QL+AAwA5gNHO+daDw3IIHHu8CySlrbxY7ZvOvnoo4+Czz+m+8WLAfbee+9g+8yBfgoxtDRPxaNZbrrppmD75o6YOKXBX/7yl5K/AMceW1gU6lvf+lbZ6zjzzDPL+svQB9LTNwlmzZrV7BDWSDO09SOo4mUR/dyDevNx+3scStM7ZIVqauArgbOcc9sDewM/MLPBwBhgmnNuIDCt+F7Uz3pI26Too3s3OaRt+rR7HLiZ3QNcWXwNdc4tNLN+wHTn3Bcq7JuJ8Z6+MyIeqxzr4GdlxkmzUmQu4EhB24ULFwbbj9+OZzv62l68uo5fzagt/KLHfkYlZCq39/vA0eT43vXMmTMn2J///OdbfR4nZIq/s3iWcgIcREraxqvmnHfeeQAceOCBwec7tMs98bWFnz0c57SP54JstNFGrfaJa/i+sz3uoG8gZceBt2sUipkNAHYBngH6OucWAhS/rD5t7DMaGF3uM1GWD4FtpG0ibIDu3SSRtilTdQFuZt2AO4EznHNLq03N6JybAEwoHiNTtZiMsub8lRHStt28qXs3OaRt+lRVgJvZ2hQK70nOOZ/Z6B0z6xc9Ki1KKshGs8022zQ7hEqkou3bb78dbN+Esu666wbfTjvt1GqfuNnp0UcfBVqSTQHMnz8fyFSzSYyf85/be9fz8ssvB7vc/Vwpj3WCpKJtnKyrXIK0n/zkJwAsW7as6mP6Jpg49UO5Jubp06cH++qrrw52Qk0na6RiJ6YVflJvAGY65y6PPpoKjCrao4B7Gh9ep0XaJov0TQ5pmyIVOzHNbD/gMeBFWh7vz6XQ3nUHsBXwBnCUc+7dCsfKxKOS/8V+8cUXgy+usWy22WZA0zox51EY7ZO4tnGnjE/RG9c+Fi0qVJ5uvPHG4IuTSOVwpZyPgW+Q43vXEw/dLLdyVNyUMWjQoGAn3Il5KClpGycuK1cDr4dYu3feeSfYXufTTz89+NqTjrZOauvEdM49DrTVsHVAvVGJVnzgnFuCtE2CV5xzvg1I+jYYaZs+mokphBA5pVPmA/fEY2njjiA/xvTpp59OPSaalA+8k5D7fOCe/v37B9uvSgWw/fbbA81pQkkzH/jOO+8cbL9CzqhRo9ravE1iPfws5Mceeyz4JkyYEGy/yk+TUD5wIYToSKgAF0KInNKpm1BOOOGEYF9//fXB9kutxYuXprUsFWpCSZIO04SSRZq1pJqfuxD/P48dOxaAnj17Bl88X8HnlveLnkPpvIgMoiYUIYToSHTqGni8Eswdd9wR7GHDhgEwZcqU4IvTSvqVahJCNfDkUA08QTr6osZNRjVwIYToSKgAF0KInNKpm1Bi4uaUX/3qV0DLArwAO+64Y7AT7tBUE0pyqAklQdSEkihqQhFCiI6EauDZQzXw5FANPEFUA08U1cCFEKIjoQJcCCFySrvWxGwAi4Hlxb8dhd409nr6V96kLNK2MrVqC9K3EtK2lFTu3VTbwAHM7Nla2yGzSJauJ0uxNIKsXU/W4qmXLF1PlmJpBGldj5pQhBAip6gAF0KInNKMAnxC5U1yRZauJ0uxNIKsXU/W4qmXLF1PlmJpBKlcT+pt4EIIIRqDmlCEECKnqAAXQoickmoBbmaHmNlsM5tnZmPSPHcjMLMtzexhM5tpZi+b2elFfy8ze9DM5hb/9qx0rARik7bJxSZtk41P+taKcy6VF9AFeBXYBlgHeAEYnNb5G3QN/YBdi/ZGwBxgMHApMKboHwNcknJc0lba5k5b6Vv/K80a+J7APOfca865FcBkYESK568b59xC59zfi/YyYCawOYXrmFjcbCJweMqhSdvkkLbJIn3rIM0CfHPgzej9gqIvl5jZAGAX4Bmgr3NuIRS+TKBPyuFI2+SQtskifesgzQK8XKrJXI5hNLNuwJ3AGc65pc2OB2mbJNI2WaRvHaRZgC8AtozebwG8leL5G4KZrU3hS5rknPOrHr9jZv2Kn/cDFqUclrRNDmmbLNK3DtIswGcAA81sazNbBzgGmJri+evGzAy4AZjpnLs8+mgqMKpojwLuSTk0aZsc0jZZpG89pNxbO5xCD+2rwHnN7j2uIf79KDze/RN4vvgaDmwCTAPmFv/2akJs0lba5k5b6VvfS1PphRAip2gmphBC5BQV4EIIkVNUgAshRE5RAS6EEDlFBbgQQuQUFeBCCJFTVIALIURO+f8VX3mm6lruAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(\"The size of the training dataset is:\", len(train_images))\n",
    "\n",
    "# Pinto las primeras imagenes\n",
    "from matplotlib import pyplot\n",
    "\n",
    "for i in range(8):\n",
    "    # define subplot\n",
    "    pyplot.subplot(240 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(train_images[i], cmap=pyplot.get_cmap('gray'))\n",
    "    labels = \"label = \" + str(train_labels[i])\n",
    "    pyplot.title(labels)\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will give the appropriate shape to the training and test datasets in order to put them into the neural network. Convert the labels, which right now are numbers, into their categorical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(train_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model indicating what the training data and its labels are. Using the optimizer \"rmsprop\" and as a loss function use the categorical cross entropy.\n",
    "Then train the model for 5 epochs and a batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"rmsprop\", \n",
    "              loss = \"categorical_crossentropy\", \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 61s 1ms/step - loss: 0.1261 - accuracy: 0.9643\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 60s 1ms/step - loss: 0.0919 - accuracy: 0.9813\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.1143 - accuracy: 0.9791\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.1366 - accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.1455 - accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f6720997ad0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_images, \n",
    "          y = train_labels,\n",
    "          batch_size=4,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model with the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0929599324428651\n",
      "Test accuracy: 0.9757999777793884\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an image with a handwritting number and check the prediction. Try with several numbers ...does it work properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAABqCAYAAABUIcSXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAEy0lEQVR4nO2cT4gcRRSHv5+rOcVD1r/LZjU57EFvggQPuQaWXCKCkpwiCHtRUPBg8C54Er0uuBhBTASF5CYSPHiS3QTRJMsmS1AzZDGERBQvGnweuhc6k16nt7unet7M+6DprprZqer57Xv1quZVy8wIRp8Huu5AUI0QygkhlBNCKCeEUE4IoZzQSChJC5LWJW1IOtFWp4L7Ud15lKQp4ApwCOgBK8AxM7vcXveCLR5s8LcHgA0zuwYg6RRwBNhWKEkxux7MLTN7rL+yieubBa4Xyr287h4kLUpalbTaoK1J4peyyiYWpZK6+yzGzJaAJQiLakITi+oBc4XyXuBGs+4E29FEqBVgXtJ+SbuAo8DZdroV9FPb9ZnZXUlvAF8DU8CymV1qrWfBPdQOz2s1FmNUFc6b2fP9lbEy4YQQygkhlBOazKPcUxyfpbJp4egQFuWEEMoJE+n6PGZehUU5IYRywkS6vi1GPdIrEhblhImxKE9zpjLCopwQQjlh7F2fxzlTGWFRThh7i9rCYwBRZKBFSVqWdFPSxULdtKRvJF3Nz3uG282giuv7BFjoqzsBnDOzeeBcXg6GiZkNPIB9wMVCeR2Yya9ngPWKn2MpjjJStd3CsVr23dUdo54ws02yb2VT0uPbvVHSIrBYs50gZ+jBRGTKtkPd8Pw3STMA+flme11qF0nuIz6oL9RZ4Hh+fRw40053gm2pEAB8DmwC/5Dlm78GPEIW7V3Nz9NdBxNOA4fKwcTYZMp6Xx0vEJmynnG/hDQui66DCItygkuLKrOiYYxLozTuhUU5IYRygkvXV6Rrl5SKsCgnhFBOcOX6JmXOVEZYlBNcWdQWkxJAFAmLckII5YSRd31lAcSgoGIcXWNYlBNCKCcMdH2S5oBPgSeBf4ElM/tI0jRwmizn72fgFTO703YHm7qxtuZeTT+n6X1Usai7wNtm9gzwAvC6pGeJbNm0VElK6UtQOUP2wN8dZ8vSfeKIh4SZ5pmykvYBzwHfUzFbNjJlW2IHlrQbOA+8lJd/73v9TljU8CyqUtQn6SHgS+AzM/sqr3aTLTsOVNkfJeBjYM3MPii8FNmyKangrg6SmeSPwA/5cZga2bKMgDvbyTFKrm9sMmWHQUdZSJEp65kQygkhlBNCKCeEUE4IoZwQQjkhhHJCCOWEkU9u6ZJRSpIJi3JCCOWEEMoJIZQTUgcTt4C/8vO48Cjt3s/TZZVJf48CkLRa9nuLV1LdT7g+J4RQTuhCqKUO2hwmSe4n+RgV1CNcnxNCKCckFUrSgqR1SRuS3O3+kDQn6VtJa5IuSXozrx/6A/uTjVGSpoArZDtBesAKcMzMLifpQAvkqdszZnZB0sNkufgvAq8Ct83s/fwfcI+ZvdNm2ykt6gCwYWbXzOxv4BRwJGH7jTGzTTO7kF//CawBs2T3cTJ/20ky8VolpVCzwPVCuZfXueT/tiAB2z6wvy4phSr7Fc7l3EDSbrLdLW+Z2R8p2kwpVA+YK5T3AjcStt8KXW1BSinUCjAvab+kXcBRsq07buhyC1Lq3RyHgQ+BKWDZzN5L1ngLSDoIfAf8RPaEAIB3ycapL4CngF+Bl83sdqttxxKSD2JlwgkhlBNCKCeEUE4IoZwQQjkhhHLCf1/+bBJCWat1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_width=28\n",
    "img_height=28\n",
    "\n",
    "img = image.load_img('cuatro.png', target_size=(img_width, img_height), color_mode = \"grayscale\")\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x.astype('float32') / 255\n",
    "\n",
    "# Pinto mi nueva imagen de un numero\n",
    "x_to_plot = np.reshape(x, (28,-1))\n",
    "pyplot.subplot(240 + 1)\n",
    "pyplot.imshow(x_to_plot, cmap=pyplot.get_cmap('gray'))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2f7f9f52de93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Veo como se clasifica\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Veo como se clasifica\n",
    "y = model.predict_classes(x)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('net_numbers.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model that you just saved and make a prediction (predict_classes) with the number you just generated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 0.0929599324428651\n",
      "Test accuracy: 0.9757999777793884\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('net_numbers.h5')\n",
    "\n",
    "# miro si de verdad estoy cargando el modelo que he guardado antes \n",
    "loaded_model.summary()\n",
    "\n",
    "test_loss, test_acc = loaded_model.evaluate(test_images, test_labels, verbose = 0)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAABqCAYAAABUIcSXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAEy0lEQVR4nO2cT4gcRRSHv5+rOcVD1r/LZjU57EFvggQPuQaWXCKCkpwiCHtRUPBg8C54Er0uuBhBTASF5CYSPHiS3QTRJMsmS1AzZDGERBQvGnweuhc6k16nt7unet7M+6DprprZqer57Xv1quZVy8wIRp8Huu5AUI0QygkhlBNCKCeEUE4IoZzQSChJC5LWJW1IOtFWp4L7Ud15lKQp4ApwCOgBK8AxM7vcXveCLR5s8LcHgA0zuwYg6RRwBNhWKEkxux7MLTN7rL+yieubBa4Xyr287h4kLUpalbTaoK1J4peyyiYWpZK6+yzGzJaAJQiLakITi+oBc4XyXuBGs+4E29FEqBVgXtJ+SbuAo8DZdroV9FPb9ZnZXUlvAF8DU8CymV1qrWfBPdQOz2s1FmNUFc6b2fP9lbEy4YQQygkhlBOazKPcUxyfpbJp4egQFuWEEMoJE+n6PGZehUU5IYRywkS6vi1GPdIrEhblhImxKE9zpjLCopwQQjlh7F2fxzlTGWFRThh7i9rCYwBRZKBFSVqWdFPSxULdtKRvJF3Nz3uG282giuv7BFjoqzsBnDOzeeBcXg6GiZkNPIB9wMVCeR2Yya9ngPWKn2MpjjJStd3CsVr23dUdo54ws02yb2VT0uPbvVHSIrBYs50gZ+jBRGTKtkPd8Pw3STMA+flme11qF0nuIz6oL9RZ4Hh+fRw40053gm2pEAB8DmwC/5Dlm78GPEIW7V3Nz9NdBxNOA4fKwcTYZMp6Xx0vEJmynnG/hDQui66DCItygkuLKrOiYYxLozTuhUU5IYRygkvXV6Rrl5SKsCgnhFBOcOX6JmXOVEZYlBNcWdQWkxJAFAmLckII5YSRd31lAcSgoGIcXWNYlBNCKCcMdH2S5oBPgSeBf4ElM/tI0jRwmizn72fgFTO703YHm7qxtuZeTT+n6X1Usai7wNtm9gzwAvC6pGeJbNm0VElK6UtQOUP2wN8dZ8vSfeKIh4SZ5pmykvYBzwHfUzFbNjJlW2IHlrQbOA+8lJd/73v9TljU8CyqUtQn6SHgS+AzM/sqr3aTLTsOVNkfJeBjYM3MPii8FNmyKangrg6SmeSPwA/5cZga2bKMgDvbyTFKrm9sMmWHQUdZSJEp65kQygkhlBNCKCeEUE4IoZwQQjkhhHJCCOWEkU9u6ZJRSpIJi3JCCOWEEMoJIZQTUgcTt4C/8vO48Cjt3s/TZZVJf48CkLRa9nuLV1LdT7g+J4RQTuhCqKUO2hwmSe4n+RgV1CNcnxNCKCckFUrSgqR1SRuS3O3+kDQn6VtJa5IuSXozrx/6A/uTjVGSpoArZDtBesAKcMzMLifpQAvkqdszZnZB0sNkufgvAq8Ct83s/fwfcI+ZvdNm2ykt6gCwYWbXzOxv4BRwJGH7jTGzTTO7kF//CawBs2T3cTJ/20ky8VolpVCzwPVCuZfXueT/tiAB2z6wvy4phSr7Fc7l3EDSbrLdLW+Z2R8p2kwpVA+YK5T3AjcStt8KXW1BSinUCjAvab+kXcBRsq07buhyC1Lq3RyHgQ+BKWDZzN5L1ngLSDoIfAf8RPaEAIB3ycapL4CngF+Bl83sdqttxxKSD2JlwgkhlBNCKCeEUE4IoZwQQjkhhHLCf1/+bBJCWat1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo reconozco como un 4\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_width=28\n",
    "img_height=28\n",
    "\n",
    "img = image.load_img('cuatro.png', target_size=(img_width, img_height), color_mode = \"grayscale\")\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x.astype('float32') / 255\n",
    "\n",
    "x_to_plot = np.reshape(x, (28,-1))\n",
    "pyplot.subplot(240 + 1)\n",
    "pyplot.imshow(x_to_plot, cmap=pyplot.get_cmap('gray'))\n",
    "pyplot.show()\n",
    "\n",
    "y = loaded_model.predict_classes(x)\n",
    "print(\"Lo reconozco como un\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo reconozco como un 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Miro que pasa con las imagenes originales de test\n",
    "j = 100\n",
    "\n",
    "pyplot.subplot(240 + 1)\n",
    "test_image_to_plot = np.reshape(test_images[j], (28,-1))\n",
    "\n",
    "pyplot.imshow(test_image_to_plot, cmap=pyplot.get_cmap('gray'))\n",
    "labels = \"label = \" + str(test_labels[j])\n",
    "pyplot.title(labels)\n",
    "\n",
    "test_image = np.expand_dims(test_images[j], axis=0)\n",
    "y = loaded_model.predict_classes(test_image)\n",
    "print(\"Lo reconozco como un\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
